#=
Regression tests comparing Julia TauRunner.jl with Python TauRunner.

This script loads reference data generated by the Python version and
compares it with results from the Julia implementation.

Usage:
    julia --project=../.. compare_implementations.jl reference_data.json
=#

using JSON3
using Printf
using Statistics
using TauRunner
using TauRunner: construct_earth, radius_km, get_density, Chord, total_column_depth, x_to_r
using TauRunner.units: km, gr, cm

const TOLERANCE_RELATIVE = 0.01  # 1% relative tolerance
const TOLERANCE_ABSOLUTE = 1e-10  # Absolute tolerance for near-zero values

"""
Check if two values are approximately equal.
"""
function approx_equal(a::Real, b::Real; rtol=TOLERANCE_RELATIVE, atol=TOLERANCE_ABSOLUTE)
    if abs(a) < atol && abs(b) < atol
        return true
    end
    return abs(a - b) / max(abs(a), abs(b)) < rtol
end

"""
Compare geometry results between Python and Julia.
"""
function compare_geometry(ref_geometry)
    println("\n" * "=" ^ 60)
    println("GEOMETRY COMPARISON")
    println("=" ^ 60)

    earth = construct_earth()
    all_passed = true

    # Check Earth radius
    julia_radius_km = radius_km(earth)
    python_radius_km = ref_geometry["earth_radius_km"]
    radius_match = approx_equal(julia_radius_km, python_radius_km)

    println("\nEarth Radius:")
    println("  Python: $python_radius_km km")
    println("  Julia:  $julia_radius_km km")
    println("  Match:  ", radius_match ? "✓ PASS" : "✗ FAIL")
    all_passed &= radius_match

    # Check density profile
    println("\nDensity Profile:")
    println("  r        Python (g/cm³)  Julia (g/cm³)   Match")
    println("  " * "-" ^ 55)

    for entry in ref_geometry["density_profile"]
        r = entry["r"]
        python_rho = entry["density_gcm3"]
        julia_rho = get_density(earth, r) / (gr / cm^3)
        match = approx_equal(julia_rho, python_rho)
        all_passed &= match

        status = match ? "✓" : "✗"
        @printf("  %.1f      %12.4f    %12.4f    %s\n", r, python_rho, julia_rho, status)
    end

    # Check chord properties
    println("\nChord Properties:")

    for chord_test in ref_geometry["chord_tests"]
        theta_deg = chord_test["theta_deg"]
        println("\n  θ = $(theta_deg)°:")

        # Create Julia chord
        chord = Chord(theta_deg * π / 180)

        # Compare total length
        python_length = chord_test["total_length"]
        julia_length = chord.total_length
        length_match = approx_equal(julia_length, python_length)
        all_passed &= length_match

        status = length_match ? "✓" : "✗"
        @printf("    Total length:  Python=%.6f  Julia=%.6f  %s\n",
                python_length, julia_length, status)

        # Compare column depth
        python_depth = chord_test["column_depth_gcm2"]
        julia_depth = total_column_depth(chord, earth) / (gr / cm^2)
        depth_match = approx_equal(julia_depth, python_depth)
        all_passed &= depth_match

        status = depth_match ? "✓" : "✗"
        @printf("    Column depth:  Python=%.4e  Julia=%.4e  %s\n",
                python_depth, julia_depth, status)

        # Compare x_to_r
        print("    x_to_r: ")
        xr_all_match = true
        for xr in chord_test["x_to_r"]
            x = xr["x"]
            python_r = xr["r"]
            julia_r = x_to_r(chord, x)
            if !approx_equal(julia_r, python_r)
                xr_all_match = false
                all_passed = false
            end
        end
        println(xr_all_match ? "✓ All match" : "✗ Some mismatches")
    end

    return all_passed
end

"""
Compare Monte Carlo results between Python and Julia.
"""
function compare_monte_carlo(ref_mc)
    println("\n" * "=" ^ 60)
    println("MONTE CARLO COMPARISON")
    println("=" ^ 60)

    if isempty(ref_mc)
        println("\nNo Monte Carlo reference data available.")
        println("Run generate_python_reference.py without --skip-mc to generate.")
        return true
    end

    # Check if cross-sections are loaded
    println("\nNote: Full MC comparison requires cross-section data to be loaded.")
    println("Currently showing reference data statistics:\n")

    for test in ref_mc
        energy = test["energy_eV"]
        theta = test["theta_deg"]
        n_events = test["n_events"]

        println("  E = $(energy) eV, θ = $(theta)°, N = $(n_events):")
        println("    ν_τ survived: $(test["nutau_survived"])")

        if test["nutau_survived"] > 0
            @printf("    ν_τ <E_out>:  %.4e eV\n", test["nutau_mean_energy_out"])
        end

        if test["tau_exited"] > 0
            println("    τ exited: $(test["tau_exited"])")
            @printf("    τ <E_out>:   %.4e eV\n", test["tau_mean_energy_out"])
        end

        @printf("    <n_CC>: %.2f, <n_NC>: %.2f\n", test["mean_n_cc"], test["mean_n_nc"])
        println()
    end

    # TODO: When cross-sections are fully implemented, add actual comparison
    println("  [Full MC comparison will be enabled when cross-sections are loaded]")

    return true  # Don't fail on MC until implemented
end

"""
Run all regression tests.
"""
function run_regression_tests(reference_file::String)
    println("Loading reference data from: $reference_file")

    if !isfile(reference_file)
        error("Reference file not found: $reference_file")
    end

    ref_data = JSON3.read(read(reference_file, String))

    println("Reference data version: $(ref_data["version"])")

    # Run comparisons
    geometry_passed = compare_geometry(ref_data["geometry"])
    mc_passed = compare_monte_carlo(ref_data["monte_carlo"])

    # Summary
    println("\n" * "=" ^ 60)
    println("SUMMARY")
    println("=" ^ 60)
    println("  Geometry tests: ", geometry_passed ? "✓ PASSED" : "✗ FAILED")
    println("  Monte Carlo tests: ", mc_passed ? "✓ PASSED" : "✗ SKIPPED/PASSED")
    println()

    all_passed = geometry_passed && mc_passed

    if all_passed
        println("All regression tests PASSED!")
    else
        println("Some regression tests FAILED!")
        exit(1)
    end

    return all_passed
end

# Main entry point
if abspath(PROGRAM_FILE) == @__FILE__
    if length(ARGS) < 1
        println("Usage: julia compare_implementations.jl <reference_data.json>")
        println("\nGenerate reference data first:")
        println("  python generate_python_reference.py --output reference_data.json")
        exit(1)
    end

    run_regression_tests(ARGS[1])
end
